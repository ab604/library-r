# Wrangling bibliometrics data {#sec-wrangling-bibliometrics-data}

**WORK IN PROGRESS**

::: {#bibliometrics-summary .callout-important .unnumbered title="Summary" appearance="simple" icon="false"}
The bibliometrics team have a spreadsheet detailing published journal articles from which they would like to understand, *"Which UoS department is affiliated to each article?"*

Here we go through the process of wrangling the data to answer this question and produce some summary statistics.
:::

It's time to apply our knowledge to some bibliometrics data.

```{r}
#| message: false
#| warning: false
#| label: load-libraries
library(tidyverse)
#library(openxlsx)
library(janitor)
```

## Describing the problem

I'll attempt to describe the problem as I understand it, and then break it down into parts that correspond with @sec-getting-started , @sec-data-wrangling-i and @sec-data-wrangling-ii.

Here's an excerpt from an email Kate sent me on 2023-11-21:

> \[A spreadsheet\] is produced from a fortnightly check... for REF and Funder Compliance... the Export Control Team periodically asks us for information about particular staff/institutions/countries and...trying to search in the Affiliations can be very time consuming...

From what I can see, the overarching problem is this

:::{.callout-note appearance="simple" icon="false"}
1. There are three data sources: Scoupus, Web of Science and Pure, from which tables about publications are regularly downloaded.
2. The data contained in these three tables needs to be joined, and then joined to other data.
3. The final output is a table indicating the open access status, funding body compliance status and University department affiliation for each publication.
:::

I'm not going to be able to cover every step, For some of these steps we need to create our own tables of data e.g a table of department codes or open access codes, as well as download pre-existing data. 

I downloaded some data as a `csv` file from [Scopus Advanced Search](https://www.scopus.com/search/form.uri?display=advanced) to generate a report for 31 days prior to 2024-03-13 using the following query string:

```         
AF-ID("University of Southampton" 60025225) AND PUBYEAR > 2017 AND PUBYEAR < 2025 AND ( LIMIT-TO ( DOCTYPE,"ar" ) OR LIMIT-TO ( DOCTYPE,"re" ) OR LIMIT-TO ( DOCTYPE,"cp" ) OR LIMIT-TO ( DOCTYPE,"ed" ) OR LIMIT-TO ( DOCTYPE,"le" ) ) AND RECENT(31)
```

I'll read it in directly from the repository for this book, and use `glimpse()` to look at the contents. It has 483 rows and 20 columns, each row comprises a set of observations for a single publication as per @fig-tidy-data, the title of each publication is in the `Title` column.

```{r}
#| label: load-scopus
#| message: false
# Load the scopus csv file
scopus_dat <- read_csv("https://github.com/ab604/library-r/raw/main/data/scopus-2024-03-13.csv")
# Look at the contents
glimpse(scopus_dat)
```

So let's assume the question here is:

> "Which University of Southampton Department(s) are affiliated with each publication?"

The departments are in the `Affiliations` column. Using two more `dplyr` verbs, `slice()` and `pull()`, let's look at the first four values (@lst-slice-affiliation).

::: {.callout-note appearance="simple" icon="false"}
Note how I've used an integer vector `1:4` as the argument to `slice()` to indicate I want rows 1 to 4, and then the column variable of interest as the argument for `pull()` to pull out the values for the `Affiliations` in rows 1 to 4.
:::

This yields a character vector with four values and I can immediately see that `Affiliations` does contain not a single tidy value in each cell, it's a **messy** value with multiple values separated by semi-colons.

```{r}
#| lst-label: lst-slice-affiliation
#| lst-cap: Slice and pull the first four Affiliation values 
scopus_dat |>         
  slice(1:4) |>       
  pull(Affiliations)  
```

To be tidy, each semi-colon separated value should be in their own column. So we need to tidy the data (@sec-tidy-data, @fig-tidy-data ).

## Tidying the Scopus data

Let's break the problem down:

1.  First let's separate the multiple affiliations that are in a single column into multiple columns, one for each affiliation. This is in keeping with tidy rule that each value should have it's own cell.

We can use `tidyr` package function [`separate_wider_delim()`](https://tidyr.tidyverse.org/reference/separate_wider_delim.html).

`separate_wider_delim()` takes three necessary arguments, and has several other optional arguments. The necessary ones are a data frame, here that's `scopus_dat`. `cols` the column(s) we are separating, here that's `Affiliations` and `delim` is a string we using as a delimiter to split the values on which as we saw is going to be `;`.

As this is going to create new column variables, these need names too and here I provide the string for underscore `"_"` to `names_sep` and each new variable will have an underscore and numerical suffix called `Affiliations_1`, `Affiliations_2` etc.

The final argument we'll use is `too_few` and we'll use `"align_start"` as the value. This means that our departments will first be put in `Affiliations_1` and adds missing values columns `NA` in any columns where a row runs out of departments. This ensures all rows have the same number of columns regardless of how many values were in the original Affiliations column. The total number of Affiliation columns will be determined by the article with the most Affiliation values. 

Hence the table will get wider as new columns are created!

I'll assign the output to a new data frame object `scopus_dat_wide`.

```{r}
#| lst-label: lst-sep-wide-delim
#| lst-cap: Separating the Affiliation values in `scopus_dat` into individual columns using `separate_wider_delim()`. 
scopus_dat_wide <- scopus_dat |> 
  separate_wider_delim(cols = Affiliations, 
                       delim = ";", 
                       names_sep = "_", 
                       too_few = "align_start")
```

I'll leave it for you to examine, but you should see we have 311 Affiliation variables now, many of the contain missing values `NA`. We still have 483 rows, but 330 variables in total.

```{r}
#| eval: false
glimpse(scopus_dat_wide) 
```

2. How do we get rid of all those missing values? We can fix that by changing the shape of the data to long with `pivot_longer()` and dropping any `Affiliation` values that are `NA`.

Concretely, we have 311 Affiliation columns, if we pivot the table such that those column names become values in rows, the values from those columns fill a new single column variable, here `Department`, and we create a long table (with lots of duplicated information). And we can simply drop any row where the value of `Department` is `NA`.

As we have to say which columns we want to take the values from to fill `Department` and all our columns of interest begin with `Affliation_`, we can use the handy `dpylr` select function [`starts_with()`](https://dplyr.tidyverse.org/reference/select.html) to help us.

The name of each `Affiliation_` column will be transformed to values in a column I'm going to call `Affiliation_No`.

This yields a table with 3722 rows and 21 columns, one more column than we started with in the original `scopus_dat` table.

```{r}
#| lst-label: lst-scopus-pivot-long
#| lst-cap: Separating the Affiliation values in `scopus_dat` into individual columns using `separate_wider_delim()`. 

scopus_dat_long <- scopus_dat_wide |> 
  pivot_longer(starts_with("Affiliations_"),
               names_to = "Affiliation_No", 
               values_to = "Department", 
               values_drop_na = TRUE)
```

3. But we're only interested in the University of Southampton affiliations, so many of these rows aren't of use to us.

```{r}
#glimpse(scopus_dat_long)

scopus_dat_long_soton <- scopus_dat_long |> 
  filter(str_detect(Department,"Southampton"))

#scopus_dat_long_other <-  scopus_dat_long |> 
 # filter(!str_detect(Department,"Southampton"))
```
## Web of science

`https://www.webofscience.com/wos/woscc/summary/1d7d2706-f966-4c7c-8cbd-85de6e118dc2-d8d761f7/relevance/1`

```{r}
wos_dat <- read_csv("https://github.com/ab604/library-r/raw/main/data/web-of-science-2024-03-26.csv", skip = 3)

dat_join <- scopus_dat_long_soton |> left_join(wos_dat, by = "DOI")
```


```{r}
#| message: false
departments <- read_csv("https://github.com/ab604/library-r/raw/main/data/faculty-and-department-codes-2024-03-18.csv")

dept_names <- departments |> pull(department_name) |> glue::glue_collapse(sep = "|")

glimpse(dept_names)

soton_depts <- scopus_dat_long_soton |> 
  mutate(department_name = str_extract(Department,dept_names))
```


### Our workflow

Let's consider the steps we need to take to arrive at a table which is tidy and contains the values for each UoS affiliation for each published article:

1.  Import the table, check it and repair if necessary.
2.  Separate the affiliations into a new column (variable) for each affiliation associated with each article.

The articles don't all have the same number of affiliations, so we will have lots of columns with only a few values in. In other words a wide table (lots of columns) but sparsely populated. What we want is a table that has only the UoS affiliation observations.

What is our usual trick for obtaining only the observations of interest?

We `filter` the rows. But hang on, we can't do that unless we have the each affiliation in a row, rather spread across lots of columns. So what do?

Transforming a table so that columns become rows, or rows become columns is called ***pivoting***. Here, having separated the affiliations and created a wide table with a variable for each affiliation we want to pivot from wide-to-long.

When we pivot wide-to-long, the column names become a new set of observations in a new variable, and the values from each column become another set of observations in a new variable. Everything else is duplicated and hence we end up with lots of rows.

3.  Using `pivot_long` we create a table on which we can filter the rows for the UoS affiliations.

For some articles there may be multiple UoS affiliations, so we may wish to pivot long-to-wide to create a final tidy table with a single row for each article, but columns for each UoS affiliation.

### Importing

Let's start with a small data set called `OA-compliance-subset-2023-11-29.xlsx` from the repository

```{r}
#| label: import-data
# Use the read.xlsx to download and load the excel file into object dat
dat <- read.xlsx("https://github.com/ab604/library-r/raw/main/data/OA-compliance-subset-2023-11-29.xlsx")
```

Use the `glimpse()` function from `dplyr` to take a peak at this data:

1.  We can see that the table has 20 rows with 35 columns of variables.
2.  Looking at the data types, we can see the import function parsed two types of data: `<chr>` character, meaning strings of text and `<dbl>` double, meaning numerical data. However, looking at the variable names, some are dates and haven't been identified automatically.
3.  There are lots of `NA` s, which are missing values. This may or may not matter.

```{r}
#| label: glimpse-data
glimpse(dat)
```

### Let's fix the dates

We'll make a new object called `dat_repaired` and use the `mutate` function from `dpylr` along with `convert_to_date` in the `janitor` package to convert the number values into dates.

Then we'll take a look with the glimpse function and we can see these look like dates in the year-month-day format and are identified as `<date>` data types.

```{r}
#| label: fix-dates
dat_repaired <- dat |> mutate(Report.Date = convert_to_date(Report.Date),
                              Accepted.Date = convert_to_date(Accepted.Date),
                              Epublication.Date = convert_to_date(Epublication.Date))
glimpse(dat_repaired)
```

```{r}
dat_repaired |> select(Affiliations) |> slice(1:2)
```
