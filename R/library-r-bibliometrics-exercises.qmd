---
title: "R for Librarians: Bibliometrics Exercises"
author: "Alistair Bailey"
date: last-modified
date-format: "[Last Updated on] YYYY-MM-DD"
format: 
    html:
      theme: 
         light: [cosmo, ../styles.scss]
         dark: [cosmo, ../dark.scss]
      code-fold: true
      code-link: true
      code-overflow: wrap
      highlight: github
      embed-resources: true
      toc: true
      page-layout: full
    # pdf:
    #   toc: true
    #   number-sections: true
    #   colorlinks: true
knitr:
  opts_chunk:
    results: hide
execute: 
  eval: true
  cache: true
---

## Background

To practice our data wrangling skills we're going to work with some real Pure, Scopus and Web of Science data.

The overall aim is to be able to understand which publications in these tables are REF and Funder compliant, and which University of Southampton department each publication is associated with via the author affiliations.

## Set-up

We're going to need to load the following tidyverse packages to work with the data.

```{r}
#| label: set-up
#| message: false
#| warning: false
#| code-fold: false
library(tidyverse)
library(readxl)
library(janitor)
```

```{r}
#| label: flextable
#| echo: false
#| warning: false
library(flextable)
```

## Pure data

### Download and inspection

Download this Excel spreadsheet and open it up in Excel.

```{r}
#| label: download-pure-data
#| eval: false
#| code-fold: show

# This is the URL for the excel file
pure_url <- "https://github.com/ab604/library-r/raw/main/data/pure-2024-03-26.xls"
# This is the name of the excel file
pure_file <- "pure-2024-03-26.xls"

# This downloads the file to whatever directory you are in, 
# but only if "pure-2024-03-26.xls" doesn't already exist.
if (!file.exists(pure_file)) {
  download.file(pure_url,pure_file, mode = "wb")
}
```

1.  What's the first thing you notice about the column variable headings?

::: {.callout-warning collapse="true" title="Expand this block to see my thoughts"}
1.  Many of the variable names are non-syntactic.
2.  Many of the variable names are very long.
3.  We have no code book to tell us what they represent.

As described in [Section 3.5.2](https://ab604.github.io/library-r/getting-started.html#sec-names), R has rules about what constitutes a valid name. A syntactic name must not have white space and must contain letters, digits, `.` and `_`, but cannot begin with a digit or underscore. There are also certain reserved names such as TRUE that cannot be used. A name that breaks these rules is non-syntactic and can only be called if encased with back ticks.

For example the first variable would have be called in R like so:

\`Electronic version(s) of this work \> DOI (Digital Object Identifier)-0\`

So what should we do? I suggest we change them to something easier to work with.

But how? And to what?
:::

2.  Load the data in using the `read_xls()` function and create an object called `pure`

```{r}
#| label: load-data
#| message: false
pure <- read_xls("pure-2024-03-26.xls")
```

3.  Using whatever method you wish, inspect `pure` and find out how many observations and how many columns there are.

::: {.callout-note collapse="true" title="Expand this block to see what I did"}
Looking at my environment pane I see that `pure` has 5186 rows and 21 columns.
:::

4.  The simplest way to deal with the columns is to use the `clean_names()` function from the `janitor` package. Assign a new object called `pure_cln` as the output of `pure` piped to `clean_names()`

Inspect and compare `pure` and `pure_cln`. In what way are they different?

```{r}
#| label: clean-the-names
pure_cln <- pure |> 
  clean_names()
```

5.  I'm still not happy with the name of column one, so one way to rename things is using `rename()`. The format is `rename(NEW_NAME = OLD_NAME)`. Note that the new name has to be typed, but the old name can be filled with tab completion.

Rename `electronic_version_s_of_this_work_doi_digital_object_identifier_0` as `doi`. You can re-assign `pure_cln` to overwrite this.

```{r}
#| label: check-dupes
pure_cln <- pure_cln |> 
  rename(doi = electronic_version_s_of_this_work_doi_digital_object_identifier_0)
```

## Wrangle

Ok, so it's still quite hard to make sense of what's going on with this data. We've got several thousand rows and 21 different variables. Do we need them all?

Let's look at the column names:

```{r}
#| label: pure-columns
#| code-fold: false
#| results: hold
colnames(pure_cln)
```

I'm going to arbitrarily select the ones I think are most useful and in doing so rename some of them. `select()` can rename variables at the same time in

1.  Select these six columns in this order and rename 6 to `title`, 18 to `journal` and 12 to `licence`.

**Don't forget to tab-complete**

```         
[1] "doi"                                                     
[6] "title_of_the_contribution_in_original_language_5" 
[18] "journal_journal_17" 
[12] "electronic_version_s_of_this_work_licence_to_document_11"
[14] "open_access_status_13" 
[10] "workflow_step_9"
```

```{r}
#| label: select-pure-cols
pure_sel <- pure_cln |> 
  select(doi, 
         title = title_of_the_contribution_in_original_language_5,
         journal = journal_journal_17, 
         open_access_status_13, ref_open_access_compliance_status_14,
         licence = electronic_version_s_of_this_work_licence_to_document_11,
         workflow_step_9)
```

2.  Have a look at `pure_sel` when you are done using which method you prefer.

```{r}
#| label: glimpse-pure-sel
glimpse(pure_sel)
```

3.  Let's do some summarising.

3.1. How many different types of open access status, ref compliance, licences, and workflow statuses are there?

```{r}
#| label: count-types
# Distinct open access status
pure_sel |> 
  distinct(open_access_status_13)
# Distinct REF status
pure_sel |> 
  distinct(ref_open_access_compliance_status_14)
# Distinct licences. What the hell?
pure_sel |> 
  distinct(licence)
# Distinct workflow status
pure_sel |> 
  distinct(workflow_step_9)
```

3.2 Use `group_by()` to find out how many articles are at each `workflow_step_9` stage.

```{r}
pure_sel |> 
  group_by(workflow_step_9) |> 
  tally()
```

3.3 What's the most frequent journal in the list?

```{r}
pure_sel |> 
  group_by(journal) |> 
  tally() |> 
  arrange(desc(n))
```

3.4 Use filter to extract the article "Lipid droplets in steatotic liver disease"

```{r}
pure_lipid <- pure_sel |> 
  filter(title == "Lipid droplets in steatotic liver disease")
```

## Scopus

### Download and inspection

```{r}
#| label: download-scopus-data
#| eval: false
#| code-fold: show

# This is the URL for the Scopus CSV file
scopus_url <- "https://github.com/ab604/library-r/raw/main/data/scopus-2024-03-13.csv"
# This is the name of the excel file
scopus_file <- "scopus-2024-03-13.csv"

# This downloads the file to whatever directory you are in, 
# but only if "scopus-2024-03-13.csv" doesn't already exist.
if (!file.exists(scopus_file)) {
  download.file(scopus_url,scopus_file, mode = "wb")
}
```

Load the scopus data and fix the variable names

```{r}
#| label: load-scopus
#| message: false
scopus <- read_csv("scopus-2024-03-13.csv") |> 
  clean_names()
```

```{r}
#| label: inspect-scopus
glimpse(scopus)
```

Get the lipid article

```{r}
scopus_lipid <- scopus |> 
  filter(title == "Lipid droplets in steatotic liver disease")
```

Introducing `pull()` from so we can extract the `affilitiations` column. As we only have one row, we only get the contents of a single cell, unfortunately it contains multiple values. Can you see how each value is delimited?

```{r}
#| label: scopus-aff
#| code-fold: false
#| eval: true
scopus_lipid |> 
  pull(affiliations) 
```
We can use `tidyr` function `separate_wider_delim()` to separate the column into multiple columns. 
Below is the code for separating the `scopus_lipid` `affiliations` column with 
the following arguments:

+ `cols = affiliations` is telling the function which column to separate.

+ `names_sep = "_"` is telling the function that when it creates new columns using
the original column name `affiliation` to add an underscores before a number.

+ `delim = ";"` is telling the function to separate the string at each `;`.

+ `too_few = "align_start"` is telling the function what to do if we have a different number of values 
in each row when we separate. In our case that means different numbers of affiliations for each journal article.
Here we are saying that if a row has `too_few` wrt to another, put the first affiliation aligned to the first
column of the other rows and fill the empty columns with `NA`.
  
```{r}
#| label: scopus-sep-affiliations
#| code-fold: false 
#| eval: true
#| results: markup
scopus_lipid |> 
  select(affiliations) |> 
  separate_wider_delim(cols = affiliations,
                       names_sep = "_",
                       delim = ";",
  too_few = "align_start") |> 
  glimpse()
```





## Web of Science

```{r}
#| label: download-wos-data
#| eval: false
#| code-fold: show

# This is the URL for the WoS TSV file
wos_url <- "https://github.com/ab604/library-r/raw/main/data/web-of-science-2024-03-26.tsv"
# This is the name of the excel file
wos_file <- "web-of-science-2024-03-26.tsv"

# This downloads the file to whatever directory you are in, 
# but only if "pure-2024-03-26.tsv" doesn't already exist.
if (!file.exists(wos_file)) {
  download.file(wos_url,wos_file, mode = "wb")
}
```

```{r}
#| label: load-wos
#| eval: true
wos <- read_tsv("web-of-science-2024-03-26.tsv")
```

```{r}
#| label: inspect-wos
#| eval: false
glimpse(wos)
```

## Regular expressions

We need to learn more about working with strings to tackle the `affiliations` task. Using the [Regular expression section](https://ab604.github.io/library-r/data-wrangling-2.html#sec-regular-expressions) 
as the basis for some exercises, let's dive into working with strings.

Regular expressions are patterns, so if we wanted to 
find all the cats in vector below with names with `er` in them, then `er` would be the pattern
we are looking for.

Copy and create the `cat_names` vector:

```{r}
#| label: cats
#| code-fold: false
cat_names <-
  c(
    "Whiskers",
    "Fluffy",
    "Mittens",
    "Socks",
    "Tiger",
    "Smokey",
    "Gizmo",
    "Oreo",
    "Luna",
    "Oliver",
    "Leo",
    "Milo",
    "Charlie",
    "Simba",
    "Nala",
    "Felix",
    "Garfield",
    "Tigger",
    "Chloe",
    "Bella"
  )
```


We need the `stringr` package loaded from the tidyverse and then we'll use `str_view()` 
to view the pattern `er`. Note that regex's are strings themselves.

```{r}
cat_names |> 
  str_view("er")
```



```{r}
cat_names |> 
  str_view("er|ff")
```
